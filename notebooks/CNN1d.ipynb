{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0b76b5f2-3d6e-4af7-b97e-3ee0ecdc9422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,\\\n",
    "                                  OneHotEncoder, LabelEncoder\n",
    "\n",
    "def load_ptype_data(data_path, source, train_start='20130101', train_end='20181108',\n",
    "                    val_start='20181109', val_end='20200909',\n",
    "                    test_start='20200910', test_end='20210501'):\n",
    "    \"\"\"\n",
    "    Load Precip Type data\n",
    "    Args:\n",
    "        data_path (str): Path to data\n",
    "        source (str): Precip observation source. Supports 'ASOS' or 'mPING'.\n",
    "        train_start (str): Train split start date (format yyyymmdd).\n",
    "        train_end (str): Train split end date (format yyyymmdd).\n",
    "        val_start (str): Valid split start date (format yyyymmdd).\n",
    "        val_end (str): Valid split end date (format yyyymmdd).\n",
    "        test_start (str): Test split start date (format yyyymmdd).\n",
    "        test_end (str): Test split end date (format yyyymmdd).        \n",
    "    Returns:\n",
    "    Dictionary of Pandas dataframes of training / validation / test data\n",
    "    \"\"\"\n",
    "    \n",
    "    dates = sorted([x[-16:-8] for x in os.listdir(os.path.join(data_path))])\n",
    "    \n",
    "    data = {}\n",
    "    data['train'] = dates[dates.index(train_start) : dates.index(train_end) + 1]\n",
    "    data['val'] = dates[dates.index(val_start) : dates.index(val_end) + 1]\n",
    "    data['test'] = dates[dates.index(test_start) : dates.index(test_end) + 1]\n",
    "    \n",
    "    for split in data.keys():\n",
    "        dfs = []\n",
    "        for date in tqdm(data[split], desc=f\"{split}\"):\n",
    "            f = f\"{source}_rap_{date}.parquet\"\n",
    "            dfs.append(pd.read_parquet(os.path.join(data_path, f)))\n",
    "        data[split] = pd.concat(dfs, ignore_index=True)            \n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data, input_features, output_features, scaler_type=\"standard\", encoder_type=\"onehot\"):\n",
    "    \"\"\"\n",
    "    Function to select features and scale data for ML\n",
    "    Args:\n",
    "        data (dictionary of dataframes for training and validation data):\n",
    "        input_features (list): Input features\n",
    "        output_feature (list): Output feature\n",
    "        scaler_type: Type of scaling to perform (supports \"standard\" and \"minmax\")\n",
    "        encoder_type: Type of encoder to perform (supports \"label\" and \"onehot\")\n",
    "    Returns:\n",
    "        Dictionary of scaled and one-hot encoded data, dictionary of scaler objects\n",
    "    \"\"\"\n",
    "    scalar_obs = {\"minmax\": MinMaxScaler, \"standard\": StandardScaler}\n",
    "    scalers, scaled_data = {}, {}\n",
    "\n",
    "    scalers[\"input\"] = scalar_obs[scaler_type]()\n",
    "    scaled_data[\"train_x\"] = pd.DataFrame(scalers[\"input\"].fit_transform(data[\"train\"][input_features]),\n",
    "                                          columns=input_features)\n",
    "    scaled_data[\"val_x\"] = pd.DataFrame(scalers[\"input\"].transform(data[\"val\"][input_features]), columns=input_features)\n",
    "    scaled_data[\"test_x\"] = pd.DataFrame(scalers[\"input\"].transform(data[\"test\"][input_features]), columns=input_features)\n",
    "\n",
    "    scalers[\"output\"] = LabelEncoder()\n",
    "    scaled_data[\"train_y\"] = scalers[\"output\"].fit_transform(data['train']['precip'])\n",
    "    scaled_data[\"val_y\"] = scalers[\"output\"].transform(data['val']['precip'])\n",
    "    scaled_data[\"test_y\"] = scalers[\"output\"].transform(data['test']['precip'])\n",
    "\n",
    "    if encoder_type == \"onehot\":\n",
    "        scalers[\"output\"] = OneHotEncoder(sparse=False)\n",
    "        scaled_data[\"train_y\"] = scalers[\"output\"].fit_transform(scaled_data[\"train_y\"].reshape(len(scaled_data[\"train_y\"]), 1))\n",
    "        scaled_data[\"val_y\"] = scalers[\"output\"].transform(scaled_data[\"val_y\"].reshape(len(scaled_data[\"val_y\"]), 1))\n",
    "        scaled_data[\"test_y\"] = scalers[\"output\"].transform(scaled_data[\"test_y\"].reshape(len(scaled_data[\"test_y\"]), 1))\n",
    "\n",
    "    return scaled_data, scalers\n",
    "\n",
    "\n",
    "def reshape_data_1dCNN(data, base_variables=['TEMP_C', 'T_DEWPOINT_C', 'UGRD_m/s', 'VGRD_m/s'], n_levels=67):\n",
    "    \n",
    "    arr = np.zeros(shape=(data.shape[0], n_levels, len(base_variables))).astype('float32')\n",
    "    \n",
    "    for i, var in enumerate(base_variables):\n",
    "        \n",
    "        profile_vars = [x for x in list(data.columns) if var in x]\n",
    "        arr[:, :, i] = data[profile_vars].values.astype('float32')\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e470b6e9-6b41-410f-b847-8c5532eeb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D, Conv1D, MaxPool1D, AveragePooling1D, AvgPool1D, Activation, Input, Flatten, AveragePooling2D, MaxPool2D, LeakyReLU\n",
    "from tensorflow.keras.layers import SpatialDropout2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.losses import mean_squared_error, mean_absolute_error, binary_crossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from os.path import join\n",
    "import yaml\n",
    "\n",
    "losses = {\"mse\": mean_squared_error,\n",
    "          \"mae\": mean_absolute_error,\n",
    "          \"binary_crossentropy\": binary_crossentropy}\n",
    "\n",
    "\n",
    "class BaseConvNet1D(object):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Network consisting of sequential convolution and pooling layers. The class is built on\n",
    "    Tensorflow 2/Keras under the hood but uses the scikit-learn design paradigm to enable greater flexibility in\n",
    "    evaluating hyperparameters.\n",
    "    Attributes:\n",
    "        min_filters (int): Minimum number of convolutional filters\n",
    "        filter_growth_rate (float): Factor to scale filter count after each layer.\n",
    "        filter_width (int): Width of square convolutional filter.\n",
    "        min_data_width (int): Output of conv->pooling layer combo is flattened after the data width reaches this\n",
    "            threshold.\n",
    "        pooling_width (int): Factor by which pooling should reduce the spatial dimensions of the input.\n",
    "        hidden_activation (str): Activation function used after each convolution and dense hidden layer. leaky produces leaky relu\n",
    "        output_type: (str): Either linear (regression), sigmoid (binary classification), or softmax (multiclass)\n",
    "        pooling (str): Either max or mean\n",
    "        use_dropout (bool): If True or 1, include SpatialDropout and regular Dropout layers\n",
    "        dropoout_alpha (float): Dropout relative frequency between 0 and 1.\n",
    "        dense_neurons (int): Number of neurons in dense hidden layer. Used as information bottleneck for interpretation.\n",
    "        data_format (str): channels_last (default) or channels_first.\n",
    "        optimizer (str): Supports adam or sgd\n",
    "        loss (str): Supports mse, mae, or binary_crossentropy\n",
    "        leaky_alpha (float): If leaky activation is used, this controls the scaling factor for the leaky ReLU\n",
    "        metrics (list): List of additional metrics to calculate during training.\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "        batch_size (int): Number of examples per batch\n",
    "        verbose (int): Level of verbosity in fit loop. 1 results in a progress bar and 2 prints loss for each batch.\n",
    "        l2_alpha (float): if l2_alpha > 0 then l2 regularization with strength l2_alpha is used.\n",
    "        early_stopping (int): If > 0, then early stopping of training is triggered when validation loss does not change\n",
    "            for early_stopping epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_filters=16, filter_growth_rate=2, filter_width=5, min_data_width=4, pooling_width=2,\n",
    "                 hidden_activation=\"relu\", output_type=\"linear\",\n",
    "                 pooling=\"mean\", use_dropout=False, dropout_alpha=0.0, dense_neurons=64,\n",
    "                 data_format=\"channels_last\", optimizer=\"adam\", loss=\"mse\", leaky_alpha=0.1, metrics=None,\n",
    "                 learning_rate=0.0001, batch_size=1024, epochs=10, verbose=0, l2_alpha=0, early_stopping=0, **kwargs):\n",
    "        self.min_filters = min_filters\n",
    "        self.filter_width = filter_width\n",
    "        self.filter_growth_rate = filter_growth_rate\n",
    "        self.pooling_width = pooling_width\n",
    "        self.min_data_width = min_data_width\n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.output_type = output_type\n",
    "        self.use_dropout = use_dropout\n",
    "        self.pooling = pooling\n",
    "        self.dropout_alpha = dropout_alpha\n",
    "        self.data_format = data_format\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.loss = loss\n",
    "        self.dense_neurons = dense_neurons\n",
    "        self.metrics = metrics\n",
    "        self.leaky_alpha = leaky_alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.l2_alpha = l2_alpha\n",
    "        if l2_alpha > 0:\n",
    "            self.use_l2 = 1\n",
    "        else:\n",
    "            self.use_l2 = 0\n",
    "        self.verbose = verbose\n",
    "        self.early_stopping = early_stopping\n",
    "        self.model_ = None\n",
    "\n",
    "    def build_network(self, conv_input_shape, output_size):\n",
    "        \"\"\"\n",
    "        Create a keras model with the hyperparameters specified in the constructor.\n",
    "        Args:\n",
    "            conv_input_shape (tuple of shape [variable, y, x]): The shape of the input data\n",
    "            output_size: Number of neurons in output layer.\n",
    "        \"\"\"\n",
    "        if self.use_l2:\n",
    "            reg = l2(self.l2_alpha)\n",
    "        else:\n",
    "            reg = None\n",
    "        conv_input_layer = Input(shape=conv_input_shape, name=\"conv_input\")\n",
    "        num_conv_layers = int(np.round((np.log(conv_input_shape[1]) - np.log(self.min_data_width))\n",
    "                                       / np.log(self.pooling_width)))\n",
    "        num_filters = self.min_filters\n",
    "        scn_model = conv_input_layer\n",
    "        for c in range(num_conv_layers):\n",
    "            scn_model = Conv1D(num_filters, (self.filter_width, self.filter_width),\n",
    "                               data_format=self.data_format, kernel_regularizer=reg,\n",
    "                               padding=\"same\", name=\"conv_{0:02d}\".format(c))(scn_model)\n",
    "            if self.hidden_activation == \"leaky\":\n",
    "                scn_model = LeakyReLU(self.leaky_alpha, name=\"hidden_activation_{0:02d}\".format(c))(scn_model)\n",
    "            else:\n",
    "                scn_model = Activation(self.hidden_activation, name=\"hidden_activation_{0:02d}\".format(c))(scn_model)\n",
    "            if self.use_dropout:\n",
    "                scn_model = SpatialDropout2D(rate=self.dropout_alpha)(scn_model)\n",
    "            num_filters = int(num_filters * self.filter_growth_rate)\n",
    "            if self.pooling.lower() == \"max\":\n",
    "                scn_model = MaxPool1D(pool_size=(self.pooling_width, self.pooling_width),\n",
    "                                      data_format=self.data_format, name=\"pooling_{0:02d}\".format(c))(scn_model)\n",
    "            else:\n",
    "                scn_model = AveragePooling1D(pool_size=(self.pooling_width, self.pooling_width),\n",
    "                                             data_format=self.data_format, name=\"pooling_{0:02d}\".format(c))(scn_model)\n",
    "        scn_model = Flatten(name=\"flatten\")(scn_model)\n",
    "        if self.use_dropout:\n",
    "            scn_model = Dropout(rate=self.dropout_alpha)(scn_model)\n",
    "        scn_model = Dense(self.dense_neurons, name=\"dense_hidden\", kernel_regularizer=reg)(scn_model)\n",
    "        if self.hidden_activation == \"leaky\":\n",
    "            scn_model = LeakyReLU(self.leaky_alpha, name=\"hidden_dense_activation\")(scn_model)\n",
    "        else:\n",
    "            scn_model = Activation(self.hidden_activation, name=\"hidden_dense_activation\")(scn_model)\n",
    "        scn_model = Dense(output_size, name=\"dense_output\")(scn_model)\n",
    "        scn_model = Activation(self.output_type, name=\"activation_output\")(scn_model)\n",
    "        self.model_ = Model(conv_input_layer, scn_model)\n",
    "\n",
    "    def compile_model(self):\n",
    "        \"\"\"\n",
    "        Compile the model in tensorflow with the right optimizer and loss function.\n",
    "        \"\"\"\n",
    "        if self.optimizer.lower() == \"adam\":\n",
    "            opt = Adam(lr=self.learning_rate)\n",
    "        else:\n",
    "            opt = SGD(lr=self.learning_rate, momentum=0.99)\n",
    "        self.model_.compile(opt, losses[self.loss], metrics=self.metrics)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_shapes(x, y):\n",
    "        \"\"\"\n",
    "        Extract the input and output data shapes in order to construct the neural network.\n",
    "        \"\"\"\n",
    "        if len(x.shape) != 4:\n",
    "            raise ValueError(\"Input data does not have dimensions (examples, y, x, predictor)\")\n",
    "        if len(y.shape) == 1:\n",
    "            output_size = 1\n",
    "        else:\n",
    "            output_size = y.shape[1]\n",
    "        return x.shape[1:], output_size\n",
    "\n",
    "    def fit(self, x, y, val_x=None, val_y=None, build=True, callbacks=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \"\"\"\n",
    "        if build:\n",
    "            x_conv_shape, y_size = self.get_data_shapes(x, y)\n",
    "            self.build_network(x_conv_shape, y_size)\n",
    "            self.compile_model()\n",
    "        if val_x is None:\n",
    "            val_data = None\n",
    "        else:\n",
    "            val_data = (val_x, val_y)\n",
    "        if callbacks is None:\n",
    "            callbacks = []\n",
    "        if self.early_stopping > 0:\n",
    "            callbacks.append(EarlyStopping(patience=self.early_stopping))\n",
    "        self.model_.fit(x, y, batch_size=self.batch_size, epochs=self.epochs, verbose=self.verbose,\n",
    "                        validation_data=val_data, callbacks=callbacks, **kwargs)\n",
    "\n",
    "    def predict(self, x):\n",
    "        preds = self.model_.predict(x, batch_size=self.batch_size)\n",
    "        if len(preds.shape) == 2:\n",
    "            if preds.shape[1] == 1:\n",
    "                preds = preds.ravel()\n",
    "        return preds\n",
    "\n",
    "    def output_hidden_layer(self, x, layer_index=-3):\n",
    "        \"\"\"\n",
    "        Chop the end off the neural network and capture the output from the specified layer index\n",
    "        Args:\n",
    "            x: input data\n",
    "            layer_index (int): list index of the layer being output.\n",
    "        Returns:\n",
    "            output: array containing output of that layer for each example.\n",
    "        \"\"\"\n",
    "        sub_model = Model(self.model_.input, self.model_.layers[layer_index].output)\n",
    "        output = sub_model.predict(x, batch_size=self.batch_size)\n",
    "        return output\n",
    "\n",
    "    def saliency(self, x, layer_index=-3, ref_activation=10):\n",
    "        \"\"\"\n",
    "        Output the gradient of input field with respect to each neuron in the specified layer.\n",
    "        Args:\n",
    "            x:\n",
    "            layer_index:\n",
    "            ref_activation: Reference activation value for loss function.\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        saliency_values = np.zeros((self.model_.layers[layer_index].output.shape[-1],\n",
    "                                    x.shape[0], x.shape[1],\n",
    "                                    x.shape[2], x.shape[3]),\n",
    "                                   dtype=np.float32)\n",
    "        for s in trange(self.model_.layers[layer_index].output.shape[-1], desc=\"neurons\"):\n",
    "            sub_model = Model(self.model_.input, self.model_.layers[layer_index].output[:, s])\n",
    "            batch_indices = np.append(np.arange(0, x.shape[0], self.batch_size), x.shape[0])\n",
    "            for b, batch_index in enumerate(tqdm(batch_indices[:-1], desc=\"batch examples\", leave=False)):\n",
    "                x_case = tf.Variable(x.values[batch_index:batch_indices[b + 1]])\n",
    "                with tf.GradientTape() as tape:\n",
    "                    tape.watch(x_case)\n",
    "                    act_out = sub_model(x_case)\n",
    "                    loss = (ref_activation - act_out) ** 2\n",
    "                saliency_values[s, batch_index:batch_indices[b + 1]] = tape.gradient(loss, x_case)\n",
    "        saliency_da = xr.DataArray(saliency_values, dims=(\"neuron\", \"p\", \"row\", \"col\", \"var_name\"),\n",
    "                                   coords=x.coords, name=\"saliency\")\n",
    "        return saliency_da\n",
    "\n",
    "    def model_config(self):\n",
    "        all_model_attrs = pd.Series(list(self.__dict__.keys()))\n",
    "        config_attrs = all_model_attrs[all_model_attrs.str[-1] != \"_\"]\n",
    "        model_config_dict = {}\n",
    "        for attr in config_attrs:\n",
    "            model_config_dict[attr] = self.__dict__[attr]\n",
    "        return model_config_dict\n",
    "\n",
    "    def save_model(self, out_path, model_name):\n",
    "        model_config_dict = self.model_config()\n",
    "        model_config_file = join(out_path, \"config_\" + model_name + \".yml\")\n",
    "        with open(model_config_file, \"w\") as mcf:\n",
    "            yaml.dump(model_config_dict, mcf, Dumper=yaml.Dumper)\n",
    "        if self.model_ is not None:\n",
    "            model_filename = join(out_path, model_name + \".h5\")\n",
    "            self.model_.save(model_filename, save_format=\"h5\")\n",
    "        return\n",
    "\n",
    "\n",
    "def load_conv_net(model_path, model_name):\n",
    "    model_config_file = join(model_path, \"config_\" + model_name + \".yml\")\n",
    "    with open(model_config_file, \"r\") as mcf:\n",
    "        model_config_dict = yaml.load(mcf, Loader=yaml.Loader)\n",
    "    conv_net = BaseConvNet(**model_config_dict)\n",
    "    model_filename = join(model_path, model_name + \".h5\")\n",
    "    conv_net.model_ = load_model(model_filename)\n",
    "    return conv_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4afb576-bcf3-44c6-94ef-2aaf59a5f99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 1911/1911 [02:13<00:00, 14.32it/s]\n",
      "val: 100%|██████████| 404/404 [00:30<00:00, 13.36it/s]\n",
      "test: 100%|██████████| 184/184 [00:13<00:00, 13.66it/s]\n"
     ]
    }
   ],
   "source": [
    "mping = load_ptype_data('/glade/p/cisl/aiml/ai2es/winter_ptypes/precip_rap/mPING_converted/', 'mPING', test_end='20210430')\n",
    "# asos = load_ptype_data('/glade/p/cisl/aiml/ai2es/winter_ptypes/precip_rap/', 'ASOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fbefb718-4d7a-4c70-bea1-b13d0b5965db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.77 s, sys: 2.6 s, total: 8.38 s\n",
      "Wall time: 8.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features = []\n",
    "base_variables = ['TEMP_C', 'T_DEWPOINT_C', 'UGRD_m/s', 'VGRD_m/s']\n",
    "for i, var in enumerate(base_variables):\n",
    "\n",
    "    profile_vars = [x for x in list(mping['val'].columns) if var in x]\n",
    "    features.append(profile_vars)\n",
    "input_features = list(np.concatenate(features))\n",
    "\n",
    "\n",
    "scaled_data, scalers = preprocess_data(mping, input_features=input_features, output_features=['precip'])\n",
    "\n",
    "\n",
    "train_x = reshape_data_1dCNN(scaled_data['train_x'], base_variables=base_variables)\n",
    "train_y = scaled_data['train_y']\n",
    "val_x = reshape_data_1dCNN(scaled_data['val_x'], base_variables=base_variables)\n",
    "val_y = scaled_data['val_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ea3feed-083a-4fe0-8cd5-5282a579320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_input_layer = Input(shape=train_x.shape[1:], name=\"conv_input\")\n",
    "model = conv_input_layer\n",
    "model = Conv1D(16, 5, data_format=\"channels_last\", activation='relu', kernel_regularizer=None, padding=\"same\")(model)\n",
    "model = AveragePooling1D(pool_size=3)(model)\n",
    "model = Conv1D(32, 5, data_format=\"channels_last\", activation='relu', kernel_regularizer=None, padding=\"same\")(model)\n",
    "model = AveragePooling1D(pool_size=3)(model)\n",
    "model = Conv1D(64, 5, data_format=\"channels_last\", activation='relu', kernel_regularizer=None, padding=\"same\")(model)\n",
    "model = AveragePooling1D(pool_size=3)(model)\n",
    "model = Flatten()(model)\n",
    "model = Dense(128, activation='relu')(model)\n",
    "model = Dense(4, activation='softmax')(model)\n",
    "model = Model(conv_input_layer, model)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "24bcec09-1280-41b7-8b5a-aa6b05a41c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.4467\n",
      "Epoch 2/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3882\n",
      "Epoch 3/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3752\n",
      "Epoch 4/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3675\n",
      "Epoch 5/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3612\n",
      "Epoch 6/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3563\n",
      "Epoch 7/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3517\n",
      "Epoch 8/20\n",
      "825/825 [==============================] - 10s 12ms/step - loss: 0.3484\n",
      "Epoch 9/20\n",
      "825/825 [==============================] - 10s 12ms/step - loss: 0.3453\n",
      "Epoch 10/20\n",
      "825/825 [==============================] - 10s 12ms/step - loss: 0.3422\n",
      "Epoch 11/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3401\n",
      "Epoch 12/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3376\n",
      "Epoch 13/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3357\n",
      "Epoch 14/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3335\n",
      "Epoch 15/20\n",
      "825/825 [==============================] - 10s 12ms/step - loss: 0.3318\n",
      "Epoch 16/20\n",
      "825/825 [==============================] - 10s 12ms/step - loss: 0.3306\n",
      "Epoch 17/20\n",
      "825/825 [==============================] - 10s 12ms/step - loss: 0.3292\n",
      "Epoch 18/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3277\n",
      "Epoch 19/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3258\n",
      "Epoch 20/20\n",
      "825/825 [==============================] - 9s 11ms/step - loss: 0.3250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b67aebe1a00>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, batch_size=1024, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51c44d-4fdd-4c87-ac68-df8c62064d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-hwt]",
   "language": "python",
   "name": "conda-env-miniconda3-hwt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
