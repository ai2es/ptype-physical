{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a8b6b-423c-4699-8c1b-1383482f06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import tqdm\n",
    "import gc\n",
    "import bridgescaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from cartopy import crs as ccrs\n",
    "from cartopy import feature as cfeature\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint, CSVLogger \n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from callbacks import get_callbacks\n",
    "from metrics import average_acc\n",
    "from seed import seed_everything\n",
    "from utils import read_config\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "from plotting import plot_confusion_matrix\n",
    "from reliability import reliability_diagram, reliability_diagrams, compute_calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc3cb8-bb47-4285-9c90-0e788bbcf5d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## mPING data class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056e2ea-9ed6-46bf-b264-5c0ec82ca4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mpingData:\n",
    "    def __init__(self, \n",
    "                 conf):\n",
    "        \n",
    "        # variables and groupings\n",
    "        self.ptypes = conf['ptypes']\n",
    "        self.scaleGroups = conf['scale_groups'] \n",
    "        self.varGroups = []\n",
    "        \n",
    "        for group in self.scaleGroups:                                               \n",
    "            self.varGroups.append(conf[group])\n",
    "            \n",
    "        self.features = np.array(self.varGroups).ravel()\n",
    "            \n",
    "        # data parameters\n",
    "        \n",
    "        self.seed = conf['seed']\n",
    "        self.savePath = conf['save_loc']\n",
    "        self.mpingPath = conf['mping_path']\n",
    "        self.nSplits = conf['n_splits']\n",
    "        self.trainSize = conf['train_size']\n",
    "        \n",
    "        self.xTrain = None\n",
    "        self.yTrain = None\n",
    "        self.xValid = None\n",
    "        self.yValid = None\n",
    "        self.xTest = None\n",
    "        self.yTest = None\n",
    "            \n",
    "        # case study parameters\n",
    "        \n",
    "        self.caseStudies = conf['case_studies']\n",
    "            \n",
    "    def _split_data(self, \n",
    "                    qc, \n",
    "                    wet_bulb,\n",
    "                    split_date):\n",
    "        \n",
    "        seed_everything(self.seed)\n",
    "        all_data = pd.read_parquet(self.mpingPath)\n",
    "        \n",
    "        all_data['day'] = all_data['datetime'].apply(lambda x: str(x).split(' ')[0])\n",
    "        \n",
    "        if qc:\n",
    "            all_data = all_data[(all_data[f\"wetbulb{wet_bulb}_filter\"] == 0.0) & (all_data['usa'] == 1.0)]\n",
    "            \n",
    "        case_study_dates = []\n",
    "        for case_study in self.caseStudies:\n",
    "            for date in self.caseStudies[case_study]:\n",
    "                case_study_dates.append(date)\n",
    "        \n",
    "        test_data = all_data[all_data['datetime'].isin(case_study_dates)]\n",
    "        data = all_data[~all_data['datetime'].isin(case_study_dates)]\n",
    "        \n",
    "        tmp_data = data[data['datetime'] >= split_date]\n",
    "        test_data = pd.concat([test_data, tmp_data], ignore_index=True)\n",
    "        data = data[data['datetime'] < split_date]\n",
    "\n",
    "        splitter = GroupShuffleSplit(n_splits=self.nSplits, \n",
    "                                     train_size=self.trainSize, \n",
    "                                     random_state=self.seed)\n",
    "        train_idx, valid_idx = list(splitter.split(data, groups=data['day']))[0]\n",
    "        train_data, valid_data = data.iloc[train_idx], data.iloc[valid_idx]\n",
    "        \n",
    "        self.xTrain = train_data[self.features]\n",
    "        self.yTrain = train_data[self.ptypes]\n",
    "        self.xValid = valid_data[self.features]\n",
    "        self.yValid = valid_data[self.ptypes]\n",
    "        self.xTest = test_data[self.features]\n",
    "        self.yTest = test_data[self.ptypes]\n",
    "    \n",
    "    def _scale_data(self, \n",
    "                    scale_type):\n",
    "        \n",
    "        scale_types = {'GroupStandardScaler': bridgescaler.group.GroupStandardScaler()}\n",
    "        scaler = scale_types[scale_type] \n",
    "        \n",
    "        self.xTrain = scaler.fit_transform(x=self.xTrain, groups=self.varGroups)\n",
    "        self.xValid = scaler.transform(x=self.xValid)\n",
    "        self.xTest = scaler.transform(x=self.xTest)\n",
    "    \n",
    "    def _save_splits(self):\n",
    "        self.xTrain.to_numpy()\n",
    "        self.yTrain.to_numpy()\n",
    "        self.xValid.to_numpy()\n",
    "        self.yValid.to_numpy()\n",
    "        self.xTest.to_numpy()\n",
    "        self.yTest.to_numpy()\n",
    "        \n",
    "        np.save(f\"{self.savePath}xtrain.npy\", self.xTrain)\n",
    "        np.save(f\"{self.savePath}ytrain.npy\", self.yTrain)\n",
    "        np.save(f\"{self.savePath}xvalid.npy\", self.xValid)\n",
    "        np.save(f\"{self.savePath}yvalid.npy\", self.yValid)\n",
    "        np.save(f\"{self.savePath}xtest.npy\", self.xTest)\n",
    "        np.save(f\"{self.savePath}ytest.npy\", self.yTest)\n",
    "        \n",
    "    def preprocess(self,\n",
    "                   split=True,\n",
    "                   qc=True,\n",
    "                   wet_bulb='5.0',\n",
    "                   split_date='2021-06-01',\n",
    "                   scale=True,\n",
    "                   scale_type='GroupStandardScaler',\n",
    "                   save=True):\n",
    "        \n",
    "        if split:\n",
    "            print(\"splitting data...\")\n",
    "            self._split_data(qc, wet_bulb, split_date)\n",
    "            print(\"completed\")\n",
    "\n",
    "        if scale:\n",
    "            print(\"scaling data...\")\n",
    "            self._scale_data(scale_type)\n",
    "            print(\"completed\")\n",
    "\n",
    "        if save:\n",
    "            print(\"saving data...\")\n",
    "            self._save_splits()\n",
    "            print(\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8052f1f-9c85-4028-ab5b-21ac18be2d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35a9ae-95a6-4e6f-982c-8e7a8706e4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del mping\n",
    "    gc.collect()\n",
    "except:\n",
    "    print('pass')\n",
    "\n",
    "conf = read_config(\"config/ptype.yml\")\n",
    "\n",
    "mping = mpingData(conf)\n",
    "mping.preprocess(wet_bulb='5.0')\n",
    "# mping.preprocess(qc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cde5e9-3dc1-455d-8803-00b801d4c790",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da37cd6-22a3-4526-9284-8bca4a8f95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiLayerPerceptron:\n",
    "    def __init__(self, \n",
    "                 conf):\n",
    "        \n",
    "        # variables, groupings, data\n",
    "        self.savePath = conf['save_loc']\n",
    "        self.ptypes = conf['ptypes']\n",
    "        self.scaleGroups = conf['scale_groups'] \n",
    "        self.varGroups = []\n",
    "        \n",
    "        for group in self.scaleGroups:                                               \n",
    "            self.varGroups.append(conf[group])\n",
    "            \n",
    "        self.features = np.array(self.varGroups).ravel()\n",
    "        \n",
    "        # metric parameters\n",
    "        \n",
    "        metrics = {'average_acc': average_acc}\n",
    "        \n",
    "        self.metric = metrics[conf['metric']]\n",
    "        self.direction = conf['direction']\n",
    "        \n",
    "        # model parameters\n",
    "        \n",
    "        self.hiddenLayers = conf['model']['hidden_layers']\n",
    "        self.hiddenNeurons = conf['model']['hidden_neurons']\n",
    "        self.useDropout = conf['model']['use_dropout']\n",
    "        \n",
    "        if self.useDropout == 1:\n",
    "            self.dropoutAlpha = conf['model']['dropout_alpha']\n",
    "        \n",
    "        self.batchSize = conf['model']['batch_size']\n",
    "        self.epochs = conf['model']['epochs']\n",
    "        self.learningRate = conf['model']['lr']\n",
    "        self.activation = conf['model']['activation']\n",
    "        self.outputActivation = conf['model']['output_activation']\n",
    "        self.runEagerly = conf['model']['run_eagerly']\n",
    "        \n",
    "        optimizers = {'adam': tf.keras.optimizers.Adam(self.learningRate)}\n",
    "        losses = {'categorical_crossentropy': tf.keras.losses.CategoricalCrossentropy()}\n",
    "        \n",
    "        self.optimizer = optimizers[conf['model']['optimizer']]\n",
    "        self.loss = losses[conf['model']['loss']]\n",
    "        \n",
    "        # callback parameters\n",
    "        \n",
    "        self.callbacks = get_callbacks(conf)\n",
    "    \n",
    "    def _build_mlp_model(self):\n",
    "        input_size = np.array(self.varGroups).ravel().shape[0]\n",
    "        output_size = len(self.ptypes)\n",
    "        \n",
    "        model = tf.keras.models.Sequential()\n",
    "        \n",
    "        if self.activation == 'leaky':\n",
    "            model.add(tf.keras.layers.Dense(input_size))\n",
    "            model.add(tf.keras.layers.LeakyReLU())\n",
    "        \n",
    "            for i in range(self.hiddenLayers):\n",
    "                if self.hiddenLayers == 1:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons))\n",
    "                    model.add(tf.keras.layers.LeakyReLU())\n",
    "                else:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons))\n",
    "                    model.add(tf.keras.layers.LeakyReLU())\n",
    "                    if self.useDropout == 1:\n",
    "                        model.add(tf.keras.layers.Dropout(self.dropoutAlpha))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.Dense(input_size, activation=self.activation))\n",
    "        \n",
    "            for i in range(self.hiddenLayers):\n",
    "                if self.hiddenLayers == 1:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons, activation=self.activation))\n",
    "                else:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons, activation=self.activation))\n",
    "                    if self.useDropout == 1:\n",
    "                        model.add(tf.keras.layers.Dropout(self.dropoutAlpha))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(output_size, activation=self.outputActivation))\n",
    "        model.build((self.batchSize, input_size))\n",
    "        model.summary()\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def train(self, \n",
    "              x_train, \n",
    "              y_train, \n",
    "              x_valid, \n",
    "              y_valid):\n",
    "\n",
    "        #add option to load previous model weights\n",
    "        \n",
    "        model = self._build_mlp_model()\n",
    "        \n",
    "        model.compile(loss=self.loss, \n",
    "                      optimizer=self.optimizer, \n",
    "                      metrics=[self.metric],\n",
    "                     run_eagerly=self.runEagerly)\n",
    "        \n",
    "        model.fit(x_train, \n",
    "                  y_train, \n",
    "                  validation_data=(x_valid, y_valid), \n",
    "                  callbacks=self.callbacks,\n",
    "                  batch_size=self.batchSize,  \n",
    "                  epochs=self.epochs)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def load_saved_model(self,\n",
    "                        load_path=None):\n",
    "        \n",
    "        if load_path is None:\n",
    "            load_path = f\"{self.savePath}best_weights.h5\"\n",
    "            \n",
    "        print(f\"loading from {load_path}\")\n",
    "        \n",
    "        model = self._build_mlp_model()\n",
    "        model.load_weights(load_path)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict(self, \n",
    "                model, \n",
    "                x_test, \n",
    "                y_test):\n",
    "        \n",
    "        predictions = model.predict(x_test)\n",
    "\n",
    "        probs = np.max(predictions, 1)\n",
    "        preds = np.argmax(predictions, 1)\n",
    "        labels = np.argmax(y_test, 1)\n",
    "        \n",
    "        return labels, preds, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e40b31-bf62-4cae-a251-4edf1fbca484",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9cbdc-77e2-485c-b061-d6e5079aacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del mlp\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    print('pass')\n",
    "\n",
    "conf = read_config(\"config/ptype.yml\")\n",
    "\n",
    "x_train = np.load(f\"{conf['save_loc']}xtrain.npy\")\n",
    "y_train = np.load(f\"{conf['save_loc']}ytrain.npy\")\n",
    "x_valid = np.load(f\"{conf['save_loc']}xvalid.npy\")\n",
    "y_valid = np.load(f\"{conf['save_loc']}yvalid.npy\")\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n",
    "\n",
    "mlp = multiLayerPerceptron(conf)\n",
    "\n",
    "model = mlp.train(x_train,\n",
    "                  y_train,\n",
    "                  x_valid,\n",
    "                  y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4079a0d-dfc0-4b4f-8dc6-2430abce3749",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eaab1c-04e3-4995-80b2-aa87ee3b947b",
   "metadata": {},
   "source": [
    "### Define variables, load previous model weights, predict on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85735c79-aac1-46be-95e8-ad804c7535b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = True\n",
    "wet_bulb = '5.0'\n",
    "dataset = 'mping2021wb5'\n",
    "image_path = f\"/glade/u/home/jwillson/winter-ptype/images/{dataset}/\"\n",
    "class_names = ['ra', 'sn', 'pl', 'fzra']\n",
    "\n",
    "try:\n",
    "    del mlp\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    print('pass')\n",
    "\n",
    "conf = read_config(\"config/ptype.yml\")\n",
    "\n",
    "x_test = np.load(f\"{conf['save_loc']}xtest.npy\")\n",
    "y_test = np.load(f\"{conf['save_loc']}ytest.npy\")\n",
    "\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "mlp = multiLayerPerceptron(conf)\n",
    "model = mlp.load_saved_model()\n",
    "\n",
    "labels, preds, probs = mlp.predict(model,\n",
    "                                   x_test,\n",
    "                                   y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d969a86-d2d6-46fd-808a-a064db0ef585",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45621e8a-1b59-421f-a3c6-f63c353dea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(labels, \n",
    "                      preds, \n",
    "                      class_names, \n",
    "                      dataset,\n",
    "                      title=f\"{dataset} Confusion Matrix\", \n",
    "                      filename='cm.png')\n",
    "\n",
    "# Plot normalized confusion matrix axis=1\n",
    "plot_confusion_matrix(labels, \n",
    "                      preds, \n",
    "                      class_names, \n",
    "                      dataset, \n",
    "                      normalize=True, \n",
    "                      axis=1,\n",
    "                      title=f\"{dataset} Confusion Matrix (normalized axis=1)\", \n",
    "                      filename='cm_norm1.png')\n",
    "\n",
    "# Plot normalized confusion matrix axis=0\n",
    "plot_confusion_matrix(labels, \n",
    "                      preds, \n",
    "                      class_names, \n",
    "                      dataset, \n",
    "                      normalize=True, \n",
    "                      axis=0,\n",
    "                      title=f\"{dataset} Confusion Matrix (normalized axis=0)\", \n",
    "                      filename='cm_norm0.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f9a3e-2e45-4401-9fc1-185f7e8dc8a7",
   "metadata": {},
   "source": [
    "### Reliability Diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c39ce-2aea-48ae-8297-90128ddbbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.DataFrame.from_dict({\"pred_labels\": preds,\n",
    "                                    \"true_labels\": labels, \n",
    "                                    \"pred_conf\": probs})\n",
    "\n",
    "fig = reliability_diagram(test_data[\"true_labels\"].to_numpy(),\n",
    "                          test_data[\"pred_labels\"].to_numpy(),\n",
    "                          test_data[\"pred_conf\"].to_numpy(),\n",
    "                          num_bins=10, \n",
    "                          draw_ece=True,\n",
    "                          draw_bin_importance=\"alpha\", \n",
    "                          draw_averages=True,\n",
    "                          title=f\"{dataset} Reliability Diagram\", \n",
    "                          figsize=(5, 5), \n",
    "                          dpi=300,\n",
    "                          return_fig=True)\n",
    "\n",
    "plt.savefig(f'{image_path}reliability.png', \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ae469-6a25-4dd3-8592-3107aa0981f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptypes = [f'{dataset} Rain', f'{dataset} Snow', f'{dataset} Ice Pellets', f'{dataset} Freezing Rain']\n",
    "\n",
    "cond0 = (test_data[\"true_labels\"] == 0)\n",
    "cond1 = (test_data[\"true_labels\"] == 1)\n",
    "cond2 = (test_data[\"true_labels\"] == 2)\n",
    "cond3 = (test_data[\"true_labels\"] == 3)\n",
    "\n",
    "results = OrderedDict()\n",
    "results[ptypes[0]] = {\n",
    "    \"true_labels\": test_data[cond0][\"true_labels\"].values,\n",
    "    \"pred_labels\": test_data[cond0][\"pred_labels\"].values,\n",
    "    \"confidences\": test_data[cond0][\"pred_conf\"].values\n",
    "}\n",
    "results[ptypes[1]] = {\n",
    "    \"true_labels\": test_data[cond1][\"true_labels\"].values,\n",
    "    \"pred_labels\": test_data[cond1][\"pred_labels\"].values,\n",
    "    \"confidences\": test_data[cond1][\"pred_conf\"].values\n",
    "}\n",
    "results[ptypes[2]] = {\n",
    "    \"true_labels\": test_data[cond2][\"true_labels\"].values,\n",
    "    \"pred_labels\": test_data[cond2][\"pred_labels\"].values,\n",
    "    \"confidences\": test_data[cond2][\"pred_conf\"].values\n",
    "}\n",
    "results[ptypes[3]] = {\n",
    "    \"true_labels\": test_data[cond3][\"true_labels\"].values,\n",
    "    \"pred_labels\": test_data[cond3][\"pred_labels\"].values,\n",
    "    \"confidences\": test_data[cond3][\"pred_conf\"].values\n",
    "}\n",
    "fig = reliability_diagrams(results, \n",
    "                           num_bins=10, \n",
    "                           draw_bin_importance=\"alpha\", \n",
    "                           num_cols=2, \n",
    "                           dpi=300, \n",
    "                           return_fig=True)\n",
    "\n",
    "plt.savefig(f'{image_path}class_reliability.png', \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b20cb9-b68d-46d2-9ef0-6b26bafbd03c",
   "metadata": {},
   "source": [
    "### Cumulative accuracy vs. confidence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbbcf2-a61f-41fb-a4f4-87e0c9bb2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cov(df, \n",
    "                col = \"pred_conf\", \n",
    "                quan = \"uncertainty\", \n",
    "                ascending = False):\n",
    "    \n",
    "    df = df.copy()\n",
    "    df = df.sort_values(col, ascending = ascending)\n",
    "    df[\"dummy\"] = 1\n",
    "    df[f\"cu_{quan}\"] = df[quan].cumsum() / df[\"dummy\"].cumsum()\n",
    "    df[f\"cu_{col}\"] = df[col].cumsum() / df[\"dummy\"].cumsum()\n",
    "    df[f\"{col}_cov\"] = df[\"dummy\"].cumsum() / len(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "test_data[\"acc\"] = (test_data[\"pred_labels\"] == test_data[\"true_labels\"]).to_numpy()\n",
    "test_data_sorted = compute_cov(test_data, col = \"pred_conf\", quan = \"acc\")\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(\n",
    "    test_data_sorted[\"pred_conf_cov\"],\n",
    "    test_data_sorted[\"cu_acc\"]\n",
    ")\n",
    "ax.set_xticklabels([1.2, 1.0, 0.8, 0.6, 0.4, 0.2, 0.0])\n",
    "\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlabel(\"Test Data Fraction\", fontsize=14)\n",
    "ax2.plot(\n",
    "    test_data_sorted[\"pred_conf_cov\"],\n",
    "    test_data_sorted[\"cu_acc\"]\n",
    ")\n",
    "ax.set_ylabel(\"Cumulative Accuracy\", fontsize=14)\n",
    "ax.set_xlabel(\"Confidence\", fontsize=14)\n",
    "\n",
    "plt.savefig(f'{image_path}acc_vs_cov.png',\n",
    "            dpi=300, \n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2610d524-3420-4da2-895c-511e126e2e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax2 = ax.twiny()\n",
    "for c in [cond0, cond1, cond2, cond3]:\n",
    "    _test_data_sorted = compute_cov(test_data[c], col = \"pred_conf\", quan = \"acc\")\n",
    "    ax.plot(\n",
    "        _test_data_sorted[\"pred_conf_cov\"],\n",
    "        _test_data_sorted[\"cu_acc\"]\n",
    "    )\n",
    "    ax2.plot(\n",
    "        _test_data_sorted[\"pred_conf_cov\"],\n",
    "        _test_data_sorted[\"cu_acc\"]\n",
    "    )\n",
    "\n",
    "ax.set_xticklabels([1.2, 1.0, 0.8, 0.6, 0.4, 0.2, 0.0])\n",
    "ax.set_ylabel(\"Cumulative Accuracy\", fontsize=14)\n",
    "ax2.set_xlabel(\"Test Data Fraction\", fontsize=14)\n",
    "ax.set_xlabel(\"Confidence\", fontsize=14)\n",
    "ax.legend(ptypes)\n",
    "plt.savefig(f'{image_path}class_acc_vs_cov.png', \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14c331-75c2-4928-9fcf-ffe9b02477e3",
   "metadata": {},
   "source": [
    "### Case Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91cd9f-e4fd-4f28-8804-28362b125bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptype_list = conf['ptypes']\n",
    "case_studies = conf['case_studies']\n",
    "\n",
    "data = pd.read_parquet(conf['mping_path'])\n",
    "if qc:\n",
    "    data = data[(data[f\"wetbulb{wet_bulb}_filter\"] == 0.0) & (data['usa'] == 1.0)]\n",
    "\n",
    "case_study_dates = []\n",
    "for case_study in case_studies:\n",
    "    for date in case_studies[case_study]:\n",
    "        case_study_dates.append(date)\n",
    "        \n",
    "data = data[data['datetime'].isin(case_study_dates)]\n",
    "\n",
    "plot_types = [\"true_label\", \"pred_label\", \"pred_conf\"]\n",
    "\n",
    "def plot_case_study(df,\n",
    "                    case_study,\n",
    "                    plot_type, \n",
    "                    savefig=False, \n",
    "                    image_path=\"\"):\n",
    "    latN = 54.0\n",
    "    latS = 20.0\n",
    "    lonW = -63.0\n",
    "    lonE = -125.0\n",
    "    cLat = (latN + latS)/2\n",
    "    cLon = (lonW + lonE )/2\n",
    "    colors = {0:'lime', 1:'dodgerblue', 2:'red', 3:'black'}\n",
    "    \n",
    "    proj = ccrs.LambertConformal(central_longitude=cLon, central_latitude=cLat)\n",
    "    res = '50m'  # Coarsest and quickest to display; other options are '10m' (slowest) and '50m'.\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    ax = plt.subplot(1, 1, 1, projection=proj)\n",
    "    ax.set_extent([lonW, lonE, latS, latN])\n",
    "    ax.add_feature(cfeature.LAND.with_scale(res))\n",
    "    ax.add_feature(cfeature.OCEAN.with_scale(res))\n",
    "    ax.add_feature(cfeature.COASTLINE.with_scale(res))\n",
    "    ax.add_feature(cfeature.LAKES.with_scale(res), alpha=0.5)\n",
    "    ax.add_feature(cfeature.STATES.with_scale(res))\n",
    "    \n",
    "    first_day = str(min(df['datetime'])).split(' ')[0]\n",
    "    last_day = str(max(df['datetime'])).split(' ')[0]\n",
    "\n",
    "    zorder = [1,2,4,3]\n",
    "    if plot_type == \"pred_conf\":\n",
    "        for i in range(4):\n",
    "            sc = ax.scatter(df[\"lon\"][df[\"pred_label\"] == i]-360,\n",
    "                            df[\"lat\"][df[\"pred_label\"] == i],\n",
    "                            c=df[plot_type][df[\"pred_label\"] == i],\n",
    "                            s=60, \n",
    "                            transform=ccrs.PlateCarree(), \n",
    "                            cmap='Greys', \n",
    "                            vmin=df[plot_type].min(), \n",
    "                            vmax=df[plot_type].max())\n",
    "        \n",
    "        cbar = plt.colorbar(sc, \n",
    "                            orientation=\"horizontal\", \n",
    "                            pad=0.025, \n",
    "                            shrink=0.9325)\n",
    "        \n",
    "        cbar.set_label('Confidence', size=20)\n",
    "    \n",
    "    else:\n",
    "        for i in range(4):\n",
    "            ax.scatter(df[\"lon\"][df[plot_type] == i]-360,\n",
    "                       df[\"lat\"][df[plot_type] == i],\n",
    "                       c=df[plot_type][df[plot_type] == i].map(colors),\n",
    "                       s=60, \n",
    "                       alpha=0.2,\n",
    "                       transform=ccrs.PlateCarree(), \n",
    "                       zorder=zorder[i])\n",
    "\n",
    "        plt.legend(colors.values(), \n",
    "                   labels=[\"Rain\", \"Snow\", \"Ice Pellets\", \"Freezing Rain\"], \n",
    "                   fontsize=24, \n",
    "                   markerscale=3, \n",
    "                   loc=\"lower right\")\n",
    "    \n",
    "    titles = {\"true_label\": \"True Labels\",\n",
    "             \"pred_label\": \"Pred Labels\",\n",
    "             \"pred_conf\": \"Confidences\"}\n",
    "    \n",
    "    plt.title(f\"{dataset} {first_day} to {last_day} {titles[plot_type]}\", \n",
    "              fontsize=30)\n",
    "    \n",
    "    if savefig:\n",
    "        plt.savefig(f'{image_path}{case_study}_{plot_type}.png', \n",
    "                    dpi=300, \n",
    "                    bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "for case_study in case_studies:\n",
    "    df = data[data['datetime'].isin(case_studies[case_study])]\n",
    "    x_test = np.array(df[mlp.features])\n",
    "    y_test = np.array(df[mlp.ptypes])\n",
    "    \n",
    "    labels, preds, probs = mlp.predict(model,\n",
    "                                       x_test,\n",
    "                                       y_test)\n",
    "    df[\"pred_label\"] = preds\n",
    "    df[\"true_label\"] = labels \n",
    "    df[\"pred_conf\"] = probs\n",
    "    \n",
    "    for plot_type in plot_types:\n",
    "        plot_case_study(df,\n",
    "                        case_study,\n",
    "                        plot_type,\n",
    "                        savefig=True,\n",
    "                        image_path=image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124e4d3-189d-41ae-9e35-1266be6d9e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:winter-ptype]",
   "language": "python",
   "name": "conda-env-winter-ptype-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
