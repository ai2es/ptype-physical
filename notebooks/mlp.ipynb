{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2a8b6b-423c-4699-8c1b-1383482f06f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bridgescaler'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbridgescaler\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bridgescaler'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import tqdm\n",
    "import gc\n",
    "import bridgescaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import Callback, ModelCheckpoint, CSVLogger \n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from callbacks import get_callbacks\n",
    "from metrics import average_acc\n",
    "from seed import seed_everything\n",
    "from utils import read_config\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3c493-57be-4bf8-bc2b-225957caebf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255dce1-c45e-4394-9684-c57bb0e20758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e056e2ea-9ed6-46bf-b264-5c0ec82ca4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mpingData:\n",
    "    def __init__(self, conf):\n",
    "        # dataset\n",
    "        self.dataset = conf['dataset']\n",
    "        \n",
    "        # variables and groupings\n",
    "        self.ptypes = conf['ptypes']\n",
    "        self.scaleGroups = np.array(conf['scale_groups']) \n",
    "        \n",
    "        for i, group in enumerate(self.scaleGroups):\n",
    "            if i == 0:\n",
    "                self.varGroups = np.empty((self.scaleGroups.shape[0], \n",
    "                                           np.array(conf[group]).shape[0]), dtype='object') \n",
    "                                                            \n",
    "            self.varGroups[i] = np.array(conf[group])\n",
    "            \n",
    "        # data parameters\n",
    "        \n",
    "        self.seed = conf['seed']\n",
    "        self.savePath = conf['save_loc']\n",
    "        self.mpingPath = conf['mping_path']\n",
    "        self.nSplits = conf['n_splits']\n",
    "        self.trainSize = conf['train_size']\n",
    "        \n",
    "        self.xTrain = None\n",
    "        self.yTrain = None\n",
    "        self.xValid = None\n",
    "        self.yValid = None\n",
    "        self.xTest = None\n",
    "        self.yTest = None\n",
    "            \n",
    "        # case study parameters\n",
    "        \n",
    "        self.caseStudies = conf['case_studies']\n",
    "            \n",
    "    def _split_data(self):\n",
    "        seed_everything(self.seed)\n",
    "        all_data = pd.read_parquet(self.mpingPath)\n",
    "        \n",
    "        all_data['day'] = all_data['datetime'].apply(lambda x: str(x).split(' ')[0])\n",
    "        \n",
    "        case_studies = []\n",
    "        for case_study in self.caseStudies:\n",
    "            case_studies.append(self.caseStudies[case_study])\n",
    "            \n",
    "        case_studies = np.array(case_studies, dtype=object)\n",
    "        case_studies = np.ravel(case_studies)\n",
    "        case_study_data = all_data[all_data['datetime'].isin(case_studies)]\n",
    "        data = all_data[~all_data['datetime'].isin(case_studies)]\n",
    "\n",
    "        splitter = GroupShuffleSplit(n_splits=self.nSplits, \n",
    "                                     train_size=self.trainSize, \n",
    "                                     random_state=self.seed)\n",
    "        train_idx, valid_idx = list(splitter.split(data, groups=data['day']))[0]\n",
    "        train_data, valid_data = data.iloc[train_idx], data.iloc[valid_idx]\n",
    "        \n",
    "        features = self.varGroups.ravel()\n",
    "        self.xTrain = train_data[features]\n",
    "        self.yTrain = train_data[self.ptypes]\n",
    "        self.xValid = valid_data[features]\n",
    "        self.yValid = valid_data[self.ptypes]\n",
    "        self.xTest = case_study_data[features]\n",
    "        self.yTest = case_study_data[self.ptypes]\n",
    "    \n",
    "    def _scale_data(self, scale_type):\n",
    "        scale_types = {'GroupStandardScaler': bridgescaler.group.GroupStandardScaler()}\n",
    "        scaler = scale_types[scale_type] \n",
    "        \n",
    "        self.xTrain = scaler.fit_transform(x=self.xTrain, groups=self.varGroups)\n",
    "        self.xValid = scaler.transform(x=self.xValid)\n",
    "        self.yValid = scaler.transform(x=self.xTest)\n",
    "    \n",
    "    def _save_splits(self):\n",
    "        self.xTrain.to_numpy()\n",
    "        self.yTrain.to_numpy()\n",
    "        self.xValid.to_numpy()\n",
    "        self.yValid.to_numpy()\n",
    "        self.xTest.to_numpy()\n",
    "        self.yTest.to_numpy()\n",
    "        \n",
    "        np.save(f\"{self.savePath}{self.dataset}_xtrain.npy\", self.xTrain)\n",
    "        np.save(f\"{self.savePath}{self.dataset}_ytrain.npy\", self.yTrain)\n",
    "        np.save(f\"{self.savePath}{self.dataset}_xvalid.npy\", self.xValid)\n",
    "        np.save(f\"{self.savePath}{self.dataset}_yvalid.npy\", self.yValid)\n",
    "        np.save(f\"{self.savePath}{self.dataset}_xtest.npy\", self.xTest)\n",
    "        np.save(f\"{self.savePath}{self.dataset}_ytest.npy\", self.yTest)\n",
    "        \n",
    "    def preprocess(self, \n",
    "                   split=True,\n",
    "                   scale=True,\n",
    "                   scale_type='GroupStandardScaler',\n",
    "                   save=True):\n",
    "        if split:\n",
    "            print(\"splitting data...\")\n",
    "            self._split_data()\n",
    "            print(\"completed\")\n",
    "\n",
    "        if scale:\n",
    "            print(\"scaling data...\")\n",
    "            self._scale_data(scale_type=scale_type)\n",
    "            print(\"completed\")\n",
    "\n",
    "        if save:\n",
    "            print(\"saving data...\")\n",
    "            self._save_splits()\n",
    "            print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da37cd6-22a3-4526-9284-8bca4a8f95ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multiLayerPerceptron:\n",
    "    def __init__(self, conf):\n",
    "        # metric parameters\n",
    "        \n",
    "        metrics = {'average_acc': average_acc}\n",
    "        \n",
    "        self.metric = conf['metric']\n",
    "        self.direction = conf['direction']\n",
    "        \n",
    "        # model parameters\n",
    "        \n",
    "        self.hiddenLayers = conf['model']['hidden_layers']\n",
    "        self.hiddenNeurons = conf['model']['hidden_neurons']\n",
    "        self.useDropout = conf['model']['use_dropout']\n",
    "        \n",
    "        if self.useDropout == 1:\n",
    "            self.dropoutAlpha = conf['model']['dropout_alpha']\n",
    "        \n",
    "        self.batchSize = conf['model']['batch_size']\n",
    "        self.epochs = conf['model']['epochs']\n",
    "        self.learningRate = conf['model']['lr']\n",
    "        self.activation = conf['model']['activation']\n",
    "        self.outputActivation = conf['model']['output_activation']\n",
    "        \n",
    "        optimizers = {'adam': tf.keras.optimizers.Adam(self.learningRate)}\n",
    "        losses = {'categorical_crossentropy': tf.keras.losses.CategoricalCrossentropy()}\n",
    "        \n",
    "        self.optimizer = optimizers[conf['model']['optimizer']]\n",
    "        self.loss = losses[conf['model']['loss']]\n",
    "        \n",
    "        # callback parameters\n",
    "        \n",
    "        self.callbacks = get_callbacks(conf)\n",
    "    \n",
    "    def _build_mlp_model(self):\n",
    "        input_size = self.varGroups.ravel().shape[0]\n",
    "        output_size = self.ptypes.shape[0]\n",
    "        \n",
    "        model = tf.keras.models.Sequential()\n",
    "        \n",
    "        if activation == 'leaky':\n",
    "            model.add(tf.keras.layers.Dense(input_size))\n",
    "            model.add(tf.keras.layers.LeakyReLU())\n",
    "        \n",
    "            for i in range(num_hidden_layers):\n",
    "                if num_hidden_layers == 1:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons))\n",
    "                    model.add(tf.keras.layers.LeakyReLU())\n",
    "                else:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons))\n",
    "                    model.add(tf.keras.layers.LeakyReLU())\n",
    "                    if self.useDropout == 1:\n",
    "                        model.add(tf.keras.layers.Dropout(self.dropoutAlpha))\n",
    "        else:\n",
    "            model.add(tf.keras.layers.Dense(input_size, activation=activation))\n",
    "        \n",
    "            for i in range(num_hidden_layers):\n",
    "                if num_hidden_layers == 1:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons, activation=self.activation))\n",
    "                else:\n",
    "                    model.add(tf.keras.layers.Dense(self.hiddenNeurons, activation=self.activation))\n",
    "                    if self.useDropout == 1:\n",
    "                        model.add(tf.keras.layers.Dropout(self.dropoutAlpha))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(output_size, activation=self.outputActivation))\n",
    "        model = build_model(input_size, self.hiddenNeurons, self.hiddenLayers, output_size)\n",
    "        model.build((self.batchSize, input_size))\n",
    "        model.summary()\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    def train(self, \n",
    "              x_train, \n",
    "              y_train, \n",
    "              x_valid, \n",
    "              y_valid):\n",
    "        \n",
    "        #add preprocessing step with option to load previous splits\n",
    "\n",
    "        #add option to load previous model weights\n",
    "        \n",
    "        model = self._build_mlp_model()\n",
    "        \n",
    "        model.compile(loss=self.loss, \n",
    "                      optimizer=self.optimizer, \n",
    "                      metrics=self.metric)\n",
    "        \n",
    "        model.fit(x_train, \n",
    "                  y_train, \n",
    "                  validation_data=(x_valid, y_valid), \n",
    "                  callbacks=self.callbacks,\n",
    "                  batch_size=self.batchSize,  \n",
    "                  epochs=self.epochs)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict(self, model, x_test, y_test):\n",
    "        predictions = model.predict(x_test)\n",
    "\n",
    "        probs = np.max(predictions, 1)\n",
    "        preds = np.argmax(predictions, 1)\n",
    "        labels = np.argmax(y_test, 1)\n",
    "        \n",
    "        return self.metric(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0c24652-c984-4f8d-a63e-ded953f1db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # del mlp\n",
    "    del mping\n",
    "    # K.clear_session()\n",
    "    gc.collect()\n",
    "except:\n",
    "    print('pass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db07af7-b1fc-49e3-a1c1-9d4467c9b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = read_config(\"config/ptype.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dbcec02-3729-4356-ba80-4e036a62c15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data...\n",
      "completed\n",
      "scaling data...\n",
      "    TEMP_C_0_m  TEMP_C_250_m  TEMP_C_500_m  TEMP_C_750_m  TEMP_C_1000_m  \\\n",
      "0     3.983856      1.960746     -0.382201      0.896608       3.748605   \n",
      "1    -6.516144     -8.871194    -11.125119    -13.413846     -15.754203   \n",
      "2     1.549530      0.697966     -1.193582     -2.905500      -4.680267   \n",
      "3     4.674530      3.143140      1.326394     -0.012984      -0.898779   \n",
      "4     4.549530      3.948643      1.976451      0.439435      -0.655007   \n",
      "..         ...           ...           ...           ...            ...   \n",
      "23   22.957916     20.063769     17.687475     15.273155      12.900696   \n",
      "24   21.770416     18.354883     16.301403     14.161421      11.962797   \n",
      "25   28.145416     23.845051     21.502045     19.029873      16.543373   \n",
      "26   22.770416     22.575489     20.780939     18.774169      16.567488   \n",
      "27   26.832916     25.638598     23.583168     21.714370      19.743269   \n",
      "\n",
      "    TEMP_C_1250_m  TEMP_C_1500_m  TEMP_C_1750_m  TEMP_C_2000_m  TEMP_C_2250_m  \\\n",
      "0        4.670339       5.157393       6.114103       6.900085       6.821797   \n",
      "1      -16.441880     -17.178021     -17.626213     -17.786910     -17.886020   \n",
      "2       -6.632836      -8.522762      -9.851485     -10.952710     -11.973113   \n",
      "3       -1.810851      -2.654946      -3.688597      -5.240471      -6.860566   \n",
      "4       -1.669536      -2.636912      -3.866507      -5.404217      -6.794026   \n",
      "..            ...            ...            ...            ...            ...   \n",
      "23      10.529794       8.168724       5.841308       3.527657       1.197119   \n",
      "24       9.736834       7.541902       5.343994       3.150862       0.969958   \n",
      "25      14.081933      11.700979       9.424009       7.223715       5.012380   \n",
      "26      14.344782      12.043336       9.767562       7.485048       5.282578   \n",
      "27      17.791403      16.991327      16.778716      15.736489      14.350143   \n",
      "\n",
      "    ...  VGRD_m/s_2750_m  VGRD_m/s_3000_m  VGRD_m/s_3250_m  VGRD_m/s_3500_m  \\\n",
      "0   ...        13.655759        11.163902         9.586661         9.203067   \n",
      "1   ...        -4.960820        -4.937063        -5.224547        -5.756756   \n",
      "2   ...        -7.416077        -7.540743        -7.365028        -6.851937   \n",
      "3   ...        21.204759        22.517410        23.963854        25.339806   \n",
      "4   ...        21.031648        22.410747        23.875040        25.230304   \n",
      "..  ...              ...              ...              ...              ...   \n",
      "23  ...         3.437916         3.421315         3.247872         2.743894   \n",
      "24  ...         0.397319         0.383754         0.665836         1.200697   \n",
      "25  ...         2.357084         2.490620         2.842144         3.338904   \n",
      "26  ...        -0.139173         0.191154         1.039011         2.547082   \n",
      "27  ...        -1.796848        -1.819717        -1.737322        -1.720453   \n",
      "\n",
      "    VGRD_m/s_3750_m  VGRD_m/s_4000_m  VGRD_m/s_4250_m  VGRD_m/s_4500_m  \\\n",
      "0          9.271057         9.353870         9.272223         9.112799   \n",
      "1         -6.666258        -7.965614       -10.068389       -12.855138   \n",
      "2         -6.315477        -6.124569        -6.428867        -7.127865   \n",
      "3         26.468798        27.182482        27.680304        27.943050   \n",
      "4         26.353217        27.066375        27.427298        27.587423   \n",
      "..              ...              ...              ...              ...   \n",
      "23         2.168353         1.530590         1.092484         0.745253   \n",
      "24         1.886864         2.912728         3.823483         4.605599   \n",
      "25         3.813848         4.177095         4.411930         4.589113   \n",
      "26         4.513719         6.577255         8.266628         9.945587   \n",
      "27        -2.007759        -2.396814        -2.601415        -2.775124   \n",
      "\n",
      "    VGRD_m/s_4750_m  VGRD_m/s_5000_m  \n",
      "0          8.936383         8.821658  \n",
      "1        -16.056062       -19.082462  \n",
      "2         -8.083364        -9.124463  \n",
      "3         27.843400        27.636781  \n",
      "4         27.573186        27.343274  \n",
      "..              ...              ...  \n",
      "23         0.759070         0.808466  \n",
      "24         5.046816         5.370845  \n",
      "25         4.668712         4.676682  \n",
      "26        11.041158        11.745269  \n",
      "27        -3.040328        -4.050907  \n",
      "\n",
      "[1215426 rows x 84 columns] <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mping \u001b[38;5;241m=\u001b[39m mpingData(conf)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mmpingData.preprocess\u001b[0;34m(self, split, scale, scale_type, save)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save:\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mmpingData._scale_data\u001b[0;34m(self, scale_type)\u001b[0m\n\u001b[1;32m     67\u001b[0m scaler \u001b[38;5;241m=\u001b[39m scale_types[scale_type]\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxTrain, \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxTrain)) \n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxTrain \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvarGroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxValid \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxValid)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39myValid \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxTest)\n",
      "File \u001b[0;32m/glade/work/jwillson/conda-envs/winter-ptype/lib/python3.10/site-packages/bridgescaler/group.py:17\u001b[0m, in \u001b[0;36mGroupBaseScaler.fit_transform\u001b[0;34m(self, x, groups)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(x, groups)\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/jwillson/conda-envs/winter-ptype/lib/python3.10/site-packages/bridgescaler/group.py:23\u001b[0m, in \u001b[0;36mGroupBaseScaler.transform\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m is_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_columns_:\n\u001b[0;32m---> 23\u001b[0m     group_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_df:\n\u001b[1;32m     25\u001b[0m         transformed_x\u001b[38;5;241m.\u001b[39mloc[:, column] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_column(x[column], group_index)\n",
      "File \u001b[0;32m/glade/work/jwillson/conda-envs/winter-ptype/lib/python3.10/site-packages/bridgescaler/group.py:60\u001b[0m, in \u001b[0;36mGroupBaseScaler.find_group\u001b[0;34m(self, var_name)\u001b[0m\n\u001b[1;32m     58\u001b[0m group_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups_):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(group) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m var_name \u001b[38;5;241m==\u001b[39m group:\n\u001b[1;32m     61\u001b[0m         group_index \u001b[38;5;241m=\u001b[39m g\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m group:\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "mping = mpingData(conf)\n",
    "mping.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9465f1fe-7524-4b11-9f15-ed6a9972a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = multiLayerPerceptron(conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-evidential]",
   "language": "python",
   "name": "conda-env-miniconda3-evidential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
