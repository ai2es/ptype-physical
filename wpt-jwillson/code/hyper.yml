pbs:
  jobs: 10
  # kernel: "wptenv"
  bash: ["source ~/.bashrc", "conda activate winter-ptype"]
  batch:
    l: ["select=1:ncpus=8:ngpus=1:mem=128GB", "walltime=12:00:00", "gpu_type=v100"]
    A: "NAML0001"
    q: "casper"
    N: "echo_trial"
    o: "/glade/work/jwillson/test/echo_trial.out"
    e: "/glade/work/jwillson/test/echo_trial.err"
optuna:
  study_name: "denseNN_optimization"
  storage: "sqlite:////glade/work/jwillson/test/echo_data.db"
  reload: 0
  objective: "/glade/u/home/jwillson/ptype-physical/wpt-jwillson/code/objective.py"
  metric: "val_average_acc"
  direction: "maximize"
  n_trials: 500
  gpu: True
  save_path: '/glade/work/jwillson/test'
  sampler:
    type: "TPESampler"
    n_startup_trials: 75 
  parameters:
    trainer:num_hidden_layers:
      type: "int"
      settings:
        name: "num_hidden_layers"
        low: 1
        high: 20
    trainer:hidden_size:
      type: "int"
      settings:
        name: "hidden_size"
        low: 50
        high: 1000
    trainer:ra_weight:
      type: "float"
      settings:
        name: "ra_weight"
        low: 0.01
        high: 2.0     
    trainer:sn_weight:
      type: "float"
      settings:
        name: "sn_weight"
        low: 0.01
        high: 5.0    
    trainer:pl_weight:
      type: "float"
      settings:
        name: "pl_weight"
        low: 0.01
        high: 400.0    
    trainer:fzra_weight:
      type: "float"
      settings:
        name: "fzra_weight"
        low: 0.01
        high: 50.0    
    trainer:dropout_rate:
      type: "float"
      settings:
        name: "dropout_rate"
        low: 0.0
        high: 0.5
    trainer:learning_rate:
      type: "loguniform"
      settings:
        name: "learning_rate"
        low: 0.0000001
        high: 0.01
    trainer:activation:
      type: "categorical"
      settings:
        name: "activation"
        choices: ["relu", "leaky", "elu", "selu"]
    trainer:label_smoothing:
      type: "float"
      settings:
        name: "label_smoothing"
        low: 0.0
        high: 0.25
    trainer:batch_size:
      type: "int"
      settings:
        name: "batch_size"
        low: 1000
        high: 50000 
log:
    save_path: "/glade/work/jwillson/test/log.txt"